这个**GPT 5.2 XHIGH + Codex CLI + Plan + CSV TodoList** 的全自动开发工作流的核心在于：**利用结构化的 CSV 文件作为 AI 的“外部记忆”和“状态机”，配合强大的模型（GPT 5.2 XHIGH）进行长时间的自动闭环开发。**

以下是整理好的操作指南和优化后的提示词体系：

---

### 一、 核心逻辑与准备工作

这个玩法的核心是**不让 AI 凭空发挥，也不让 AI 自己管理进度（百分比是无效的），而是让 AI 操作一个物理文件（issue.csv）来锚定任务状态。**

#### 1. 必备文件结构
你需要准备以下三个核心文件：

*   **`AGENT.md` (系统提示词):** 定义 AI 的角色、编码规范、工具使用（MCP）规则。
*   **`plan.md` (架构蓝图):** 详细的项目需求文档、技术栈选择、架构设计。
*   **`issues.csv` (核心引擎):** 这是一个详细的任务列表，也是 AI 的状态数据库。

#### 2. `issues.csv` 的关键格式（重点！）
根据截图2中 jackliu100 的纠正，**绝对不要使用百分比进度**。CSV 必须包含明确的文本状态和验收标准。

**推荐的 CSV 表头及内容示例：**

| ID | Title (任务标题) | Content (详细内容) | Acceptance Criteria (测试验收要求) | Review Requirements (Review要求) | State (状态) | Labels (标签) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | 用户登录功能 | 实现JWT登录接口 | 1. 成功返回Token<br>2. 失败返回401 | 代码无冗余，包含注释 | **未开始** | **等待确认** |
| 2 | 数据库迁移 | 编写User表SQL | 字段包含id, user, pass | 符合范式 | **未开始** | **未开始** |

*   **State:** 只有三种状态：`未开始` (Not Started), `进行中` (In Progress), `已完成` (Completed)。
*   **Labels:** 辅助标记，如 `等待确认`。
*   **注意：** 每一行任务必须足够细致，AI 的开发边界由“Acceptance Criteria”决定，防止它跑偏。

---

### 二、 构造驱动提示词 (The Driver Prompt)

根据你的需求和佬友的经验，以下是优化后的**连续工作提示词**。请直接将此 Prompt 发送给 CODEX CLI。

**注意：** 此 Prompt 融合了状态管理、Git 提交和 MCP 测试工具调用。

#### 复制以下内容作为 Prompt：

```markdown
你是资深全栈开发专家（GPT 5.2 XHIGH），现在我们将基于本地的 `issues.csv` 文件作为任务驱动引擎，完成 `[具体项目位置目录]` 项目的代码开发与测试工作。

请严格遵守以下 **"读取-开发-测试-提交"** 的循环工作流：

### 1. 任务读取与锁定 (Task Reading & Locking)
*   **读取规则：** 读取 `issues.csv`，筛选出 `State` 为 "未开始" 且 `Labels` 为 "等待确认" 或 "未开始" 的第一条任务。
*   **状态锁定：** 在开始编写任何代码前，**必须先**将该条目的 `State` 更新为 "进行中"，`Labels` 更新为 "进行中"，并将修改后的 `issues.csv` 保存到磁盘。这是为了防止任务状态丢失。

### 2. 开发与执行 (Development & Execution)
*   **需求分析：** 仔细阅读该任务的 `Content` (内容) 和 `Acceptance Criteria` (验收标准)。
*   **代码编写：** 根据 `plan.md` 的架构规范进行编码。
*   **工具使用：** 
    *   涉及服务端测试时，必须调用指定的 MCP 工具（如 `AUTOSERVER` 或你配置的测试环境）。
    *   涉及前端时，使用指定的浏览器调试工具或模拟环境。

### 3. 验证与验收 (Verification & Review)
*   **单元测试/自测：** 执行代码并对照 `Acceptance Criteria` 进行逐条验证。
*   **Review 检查：** 对照 `Review Requirements` 检查代码质量。
*   **回归与修复：** 如果发现 Bug，立即修复，直到满足所有标准。

### 4. 状态更新与提交 (Completion & Commit)
*   **更新 CSV：** 确认任务完成后，将 `issues.csv` 中该条目的 `State` 更新为 "已完成"，`Labels` 更新为 "已完成"。
*   **Git 提交：** 执行 Git 操作：
    1. `git add .` (包含代码文件和更新后的 issues.csv)
    2. `git commit -m "feat(ID): 完成 [任务标题] - [简要说明]"`
*   **循环检查：** 完成上述步骤后，**自动** 重新读取 `issues.csv`，寻找下一条符合条件的任务。如果没有剩余任务，则报告“所有任务已完成”并停止。

---
**当前行动指令：**
请加载 `issues.csv`，开始执行上述循环。遇到任何环境报错（如缺少依赖），请自行尝试修复（Fix）并继续，除非遇到无法解决的致命错误。
```

---

### 三、 为什么这个流程能跑 11 个小时？（原理分析）

根据 jackliu100 的解释，这个流程之所以稳，是因为解决了 LLM 的两个最大弱点：

1.  **记性差（上下文溢出）：**
    *   普通的对话流，跑几个任务后 AI 就忘了最开始的需求。
    *   **本方案：** `issues.csv` 是**外部存储**。AI 不需要记住它做了什么，它只需要看 CSV 里哪一行是“未开始”的。即使上下文刷新了，CSV 文件依然记录着准确的进度。

2.  **幻觉与偷懒：**
    *   AI 喜欢假装测试通过。
    *   **本方案：** 强制要求修改 CSV 状态并 `git commit`。`git commit` 动作将代码变更和进度变更绑定在了一起。如果 AI 没写代码就改 CSV，你可以通过 Git 记录回滚；如果 AI 写了代码没改 CSV，它下一轮会重复做，从而发现错误。

3.  **模型能力（GPT 5.2 XHIGH）：**
    *   正如截图所言，普通的 GPT-4 或 Claude 3.5 可能在长程任务中会“断片”或逻辑降级。只有超高智力模型（文中指的可能是 o1 级别或特定微调版）才能在连续几十次循环中保持逻辑严密，正确理解“边界”。

### 四、 避坑指南（大佬经验总结）

1.  **CSV 是重中之重：** 不要指望 AI 一次性生成完美的 CSV。最好由人工（你）和 AI 共同讨论，确认每一行的“验收标准”清晰无误后，再启动自动驾驶模式。
2.  **测试边界：** 在 `issues.csv` 中明确指定测试环境（例如：“使用 MCP 工具 autoserver 启动服务并 curl 接口”）。如果没有这个约束，AI 可能会直接输出“测试通过”的纯文本而根本不运行代码。
3.  **成本预估：** 虽然文案中提到包月或 88codes，但这种“死循环”模式非常消耗 Token。确保你的 API 额度或套餐支持长时间高并发请求。
4.  **回归测试：** 截图提到，所有任务（State=已完成）结束后，应该有一个单独的阶段做“回归测试”，这也可以写在 CSV 的最后几行，或者单独起一个 prompt 让它遍历已完成的功能。

### 五、 总结操作步骤

1.  **Plan:** 和 AI 聊透需求，生成 `plan.md`。
2.  **Issue:** 让 AI 根据 `plan.md` 生成初步的 `issues.csv`，**人工介入修改**，确保验收标准极其具体。
3.  **Prompt:** 使用上面提供的“驱动提示词”。
4.  **Run:** 挂机，睡觉。
5.  **Check:** 早上起来看 GitHub 的 Commit 记录。